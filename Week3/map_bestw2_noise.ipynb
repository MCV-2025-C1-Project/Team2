{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63f0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from noise_filter import *\n",
    "\n",
    "from Week2.week2_histograms import SpatialPyramidHistogram\n",
    "from Week2.similarity_measures_optimized import (\n",
    "    l1_distance_matrix,\n",
    "    histogram_intersection_matrix, \n",
    "    kl_divergence_matrix,\n",
    "    normalize_hist\n",
    ")\n",
    "from Week2.mapk import mapk\n",
    "from Week2.background_remover import remove_background_morphological_gradient\n",
    "\n",
    "# --- Configuration ---\n",
    "DB_PATH = \"../Data/BBDD/\"\n",
    "QUERY_PATH = \"../Data/Week3/qsd1_w3/\"  # Changed to qsd1_w3\n",
    "GT_PATH = \"../Data/Week3/qsd1_w3/gt_corresps.pkl\"  # Changed to qsd1_w3\n",
    "CACHE_DIR = \"../Week2/best_method_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "483abb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Utility Functions\n",
    "import cv2\n",
    "\n",
    "\n",
    "def pil_to_cv2(img):\n",
    "    \"\"\"Convert PIL image to OpenCV format.\"\"\"\n",
    "    return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba492741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Best Week 2 Method (with denoised queries) ===\n",
      "Method: Pyramid Histogram Fusion\n",
      "Configuration: {'bins': (4, 4, 4), 'levels': 4, 'weights': 'uniform'}\n",
      "Found 287 database images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Denoising queries: 100%|██████████| 31/31 [00:11<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 query images (after denoising)\n",
      "Ground truth length: 30\n",
      "Computing HSV descriptors for db/BBDD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [00:36<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing HLS descriptors for db/BBDD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [00:38<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing HSV descriptors for query/denoised_images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:10<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing HLS descriptors for query/denoised_images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing similarities...\n",
      "\n",
      "Evaluating rankings...\n",
      "\n",
      "=== Results ===\n",
      "MAP@1: 0.5333\n",
      "MAP@5: 0.5833\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PYRAMID_CONFIG = {\n",
    "    \"bins\": (4, 4, 4),\n",
    "    \"levels\": 4,\n",
    "    \"weights\": \"uniform\"\n",
    "}\n",
    "\n",
    "def load_image_cv(path):\n",
    "    \"\"\"Load image in OpenCV BGR format\"\"\"\n",
    "    pil = Image.open(path).convert(\"RGB\")\n",
    "    cv_img = pil_to_cv2(pil)\n",
    "    return cv_img\n",
    "\n",
    "def filename_to_id(fname):\n",
    "    \"\"\"Convert filename to image ID\"\"\"\n",
    "    import re\n",
    "    base = os.path.splitext(os.path.basename(fname))[0]\n",
    "    m = re.search(r\"\\d+\", base)\n",
    "    return int(m.group()) if m else None\n",
    "\n",
    "def get_bounding_box_mask(mask):\n",
    "    \"\"\"Get minimum bounding rectangle from polygon mask\"\"\"\n",
    "    if mask is None or mask.size == 0 or mask.max() == 0:\n",
    "        # fallback: whole image if mask is empty\n",
    "        h, w = mask.shape if mask is not None else (0, 0)\n",
    "        rect_mask = np.ones((h, w), dtype=np.uint8)\n",
    "        return rect_mask, (0, h, 0, w)\n",
    "\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    y_idx = np.where(rows)[0]\n",
    "    x_idx = np.where(cols)[0]\n",
    "    y_min, y_max = y_idx[[0, -1]]\n",
    "    x_min, x_max = x_idx[[0, -1]]\n",
    "    rect_mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "    rect_mask[y_min:y_max+1, x_min:x_max+1] = 1\n",
    "    return rect_mask, (y_min, y_max+1, x_min, x_max+1)\n",
    "\n",
    "def build_descriptors_cached(image_list, base_path, color_space, cache_name, crop_background=False):\n",
    "    \"\"\"Build pyramid descriptors with caching (+ optional background crop).\"\"\"\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    # Make cache path include folder name to avoid collisions between different base_paths\n",
    "    folder_tag = os.path.basename(os.path.normpath(base_path))\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{cache_name}_{folder_tag}_{color_space}_bg{int(crop_background)}.pkl\")\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                cached = pickle.load(f)\n",
    "            if cached.get(\"image_list\") == image_list:\n",
    "                print(f\"Loaded {color_space} descriptors from cache ({cache_name}/{folder_tag})\")\n",
    "                return cached[\"descriptors\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Cache load failed: {e}\")\n",
    "            \n",
    "    print(f\"Computing {color_space} descriptors for {cache_name}/{folder_tag}...\")\n",
    "    pyramid = SpatialPyramidHistogram(\n",
    "        bins=PYRAMID_CONFIG[\"bins\"],\n",
    "        levels=PYRAMID_CONFIG[\"levels\"],\n",
    "        color_space=color_space,\n",
    "        weights=PYRAMID_CONFIG[\"weights\"]\n",
    "    )\n",
    "    \n",
    "    descriptors = []\n",
    "    for fname in tqdm(image_list):\n",
    "        img_path = os.path.join(base_path, fname)\n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Missing image: {img_path}\")\n",
    "        img = load_image_cv(img_path)\n",
    "\n",
    "        if crop_background:\n",
    "            # Compute polygon mask and crop to bounding rect\n",
    "            _, poly_mask, _, _ = remove_background_morphological_gradient(img)\n",
    "            rect_mask, (y1, y2, x1, x2) = get_bounding_box_mask(poly_mask)\n",
    "            img = img[y1:y2, x1:x2]\n",
    "\n",
    "        desc = pyramid.compute(img)\n",
    "        desc = normalize_hist(desc)\n",
    "        descriptors.append(desc)\n",
    "        \n",
    "    descriptors = np.vstack(descriptors) if len(descriptors) else np.empty((0, np.prod(PYRAMID_CONFIG[\"bins\"]) * (4**PYRAMID_CONFIG[\"levels\"])))\n",
    "    \n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump({\"image_list\": image_list, \"descriptors\": descriptors}, f)\n",
    "        \n",
    "    return descriptors\n",
    "\n",
    "def denoise_and_list_queries(src_folder, out_folder, threshold=40, radius_ratio=0.75,\n",
    "                             median_ksize=3, nl_h=5, nl_t=3, nl_s=21):\n",
    "    \"\"\"Denoise all images from src_folder to out_folder, keep filenames list.\"\"\"\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    names = sorted([f for f in os.listdir(src_folder) if f.lower().endswith('.jpg')])\n",
    "    for fname in tqdm(names, desc=\"Denoising queries\"):\n",
    "        noisy = cv2.imread(os.path.join(src_folder, fname), cv2.IMREAD_COLOR)\n",
    "        if noisy is None:\n",
    "            raise FileNotFoundError(f\"Cannot read image {os.path.join(src_folder, fname)}\")\n",
    "        score = fourier_noise_score(noisy, radius_ratio=radius_ratio)\n",
    "        if score > threshold:\n",
    "            img = remove_noise_median(noisy, ksize=median_ksize)\n",
    "            img = remove_noise_nlmeans(img, h=nl_h, templateWindowSize=nl_t, searchWindowSize=nl_s)\n",
    "        else:\n",
    "            img = noisy\n",
    "        cv2.imwrite(os.path.join(out_folder, fname), img)\n",
    "    return names  # filenames preserved\n",
    "\n",
    "def assert_2d_nonempty(name, arr):\n",
    "    if arr is None or arr.ndim != 2 or arr.shape[0] == 0 or arr.shape[1] == 0:\n",
    "        raise ValueError(f\"{name} must be a non-empty 2D array, got shape {None if arr is None else arr.shape}\")\n",
    "\n",
    "def evaluate_best_method():\n",
    "    print(\"=== Evaluating Best Week 2 Method (with denoised queries) ===\")\n",
    "    print(\"Method: Pyramid Histogram Fusion\")\n",
    "    print(f\"Configuration: {PYRAMID_CONFIG}\")\n",
    "    \n",
    "    # --- 0) Prepare lists ---\n",
    "    db_images = sorted([f for f in os.listdir(DB_PATH) if f.lower().endswith('.jpg')])\n",
    "    print(f\"Found {len(db_images)} database images\")\n",
    "\n",
    "    # Denoise queries from Week3 noisy folder into a temp folder (same filenames)\n",
    "    denoised_folder = \"./denoised_images\"\n",
    "    query_images = denoise_and_list_queries(QUERY_PATH, denoised_folder, threshold=40)\n",
    "\n",
    "    print(f\"Found {len(query_images)} query images (after denoising)\")\n",
    "    \n",
    "    # --- 1) Load GT (keep your path; change if you actually have Week3 GT elsewhere) ---\n",
    "    with open(GT_PATH, \"rb\") as f:\n",
    "        gt = pickle.load(f)\n",
    "    print(f\"Ground truth length: {len(gt)}\")\n",
    "    \n",
    "    # --- 2) Build descriptors ---\n",
    "    # DB (no background crop)\n",
    "    db_hsv = build_descriptors_cached(db_images, DB_PATH, \"HSV\", \"db\", crop_background=False)\n",
    "    db_hls = build_descriptors_cached(db_images, DB_PATH, \"HLS\", \"db\", crop_background=False)\n",
    "    # QUERIES (on denoised images; keep crop if you want the same ROI behavior as before)\n",
    "    query_hsv = build_descriptors_cached(query_images, denoised_folder, \"HSV\", \"query\", crop_background=True)\n",
    "    query_hls = build_descriptors_cached(query_images, denoised_folder, \"HLS\", \"query\", crop_background=True)\n",
    "\n",
    "    # Safety checks\n",
    "    assert_2d_nonempty(\"db_hsv\", db_hsv)\n",
    "    assert_2d_nonempty(\"db_hls\", db_hls)\n",
    "    assert_2d_nonempty(\"query_hsv\", query_hsv)\n",
    "    assert_2d_nonempty(\"query_hls\", query_hls)\n",
    "\n",
    "    # --- 3) Similarities ---\n",
    "    print(\"\\nComputing similarities...\")\n",
    "    sim_funcs = [l1_distance_matrix, histogram_intersection_matrix, kl_divergence_matrix]\n",
    "\n",
    "    sim_hsv = np.stack([f(query_hsv, db_hsv) for f in sim_funcs], axis=-1)\n",
    "    sim_hls = np.stack([f(query_hls, db_hls) for f in sim_funcs], axis=-1)\n",
    "\n",
    "    # Convert histogram intersection (higher = better) to a distance by negating\n",
    "    sim_hsv[..., 1] *= -1\n",
    "    sim_hls[..., 1] *= -1\n",
    "\n",
    "    # --- 4) Fuse ---\n",
    "    w1 = np.array([0.0, 1.0, 0.5])  # HSV weights\n",
    "    w2 = np.array([0.5, 0.5, 0.0])  # HLS weights\n",
    "    weighted_hsv = np.tensordot(sim_hsv, w1, axes=([2], [0]))\n",
    "    weighted_hls = np.tensordot(sim_hls, w2, axes=([2], [0]))\n",
    "    combined = 0.5 * (weighted_hsv + weighted_hls)\n",
    "\n",
    "    # --- 5) Rankings ---\n",
    "    print(\"\\nEvaluating rankings...\")\n",
    "    predictions = []\n",
    "    for q_idx in range(combined.shape[0]):\n",
    "        top_k = np.argsort(combined[q_idx])[:5]\n",
    "        pred_ids = [filename_to_id(db_images[idx]) for idx in top_k]\n",
    "        predictions.append(pred_ids)\n",
    "\n",
    "    # --- 6) MAP ---\n",
    "    map1 = mapk(gt, predictions, k=1)\n",
    "    map5 = mapk(gt, predictions, k=5)\n",
    "\n",
    "    print(f\"\\n=== Results ===\")\n",
    "    print(f\"MAP@1: {map1:.4f}\")\n",
    "    print(f\"MAP@5: {map5:.4f}\")\n",
    "    return map1, map5\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_best_method()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCV-C1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
