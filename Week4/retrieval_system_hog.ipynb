{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48e0c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Retrieval system using HOG descriptors + mAP@1 and mAP@5\n",
    "# This notebook:\n",
    "# 1. Builds (or loads) HOG descriptors for the database (BBDD)\n",
    "# 2. Extracts HOG for the query images\n",
    "# 3. Retrieves the most similar database images using cosine similarity\n",
    "# 4. Evaluates using mAP@1 and mAP@5 based on gt_corresps.pkl\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from noise_filter import preprocess_image\n",
    "import background_remover_w2 as background_remover\n",
    "from image_split import split_images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- paths (adjust to your structure) ---\n",
    "QUERY_FOLDER = \"../Data/Week4/qsd1_w4/\"     # query images\n",
    "DB_FOLDER    = \"../Data/BBDD/\"        # database (gallery) images\n",
    "GT_PATH      = \"../Data/Week4/qsd1_w4/gt_corresps.pkl\"\n",
    "\n",
    "# where to store precomputed DB descriptors\n",
    "DESC_DB_PATH = \"results/descriptors_db_hog.pkl\"\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fea714a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def compute_hog_descriptor(\n",
    "    img_bgr,\n",
    "    size=(256, 256),\n",
    "    cell=(8, 8),\n",
    "    block=(16, 16),\n",
    "    block_stride=(8, 8),\n",
    "    nbins=9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute a HOG descriptor for a BGR image.\n",
    "    We:\n",
    "    1. convert to grayscale\n",
    "    2. resize to a fixed size so all descriptors have the same length\n",
    "    3. compute HOG\n",
    "    4. L2-normalize the final vector\n",
    "    \"\"\"\n",
    "    # 1) BGR -> Gray\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    # 2) fixed resize to make descriptor length stable\n",
    "    gray = cv2.resize(gray, size)\n",
    "\n",
    "    # 3) create HOG object with consistent parameters\n",
    "    hog = cv2.HOGDescriptor(\n",
    "        _winSize=size,\n",
    "        _blockSize=block,\n",
    "        _blockStride=block_stride,\n",
    "        _cellSize=cell,\n",
    "        _nbins=nbins,\n",
    "    )\n",
    "    desc = hog.compute(gray)  # shape (N, 1)\n",
    "    desc = desc.flatten().astype(np.float32)\n",
    "\n",
    "    # 4) L2 normalization (good for cosine and distance-based retrieval)\n",
    "    norm = np.linalg.norm(desc)\n",
    "    if norm > 0:\n",
    "        desc = desc / norm\n",
    "\n",
    "    return desc\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    \"\"\"\n",
    "    Loads all .jpg/ images from a folder, sorted by filename.\n",
    "    Returns:\n",
    "        names: list of filenames\n",
    "        imgs:  list of loaded cv2 images\n",
    "    \"\"\"\n",
    "    names = sorted(\n",
    "        [\n",
    "            f\n",
    "            for f in os.listdir(folder)\n",
    "            if f.lower().endswith((\".jpg\"))\n",
    "        ]\n",
    "    )\n",
    "    imgs = []\n",
    "    for name in names:\n",
    "        path = os.path.join(folder, name)\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            print(f\"âš ï¸ Could not read {path}\")\n",
    "            continue\n",
    "        imgs.append(img)\n",
    "    return names, imgs\n",
    "\n",
    "\n",
    "def precision_at_k(retrieved_indices, gt_indices, k):\n",
    "    \"\"\"\n",
    "    Compute precision@k for one query.\n",
    "\n",
    "    retrieved_indices: list of database indices sorted by similarity (best first)\n",
    "    gt_indices: list of correct database indices for that query (can be 1 or more)\n",
    "    k: cutoff (1, 5, ...)\n",
    "\n",
    "    precision@k = (# of retrieved items in top-k that are in GT) / k\n",
    "    \"\"\"\n",
    "    retrieved_k = retrieved_indices[:k]\n",
    "    hits = sum(1 for r in retrieved_k if r in gt_indices)\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def mean_average_precision_at_k(all_retrieved, all_gts, k):\n",
    "    \"\"\"\n",
    "    Compute mean precision@k over all queries.\n",
    "    (Many assignments call this mAP@k when GT is small.)\n",
    "    \"\"\"\n",
    "    assert len(all_retrieved) == len(all_gts)\n",
    "    precisions = []\n",
    "    for retrieved, gts in zip(all_retrieved, all_gts):\n",
    "        p = precision_at_k(retrieved, gts, k)\n",
    "        precisions.append(p)\n",
    "    return np.mean(precisions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74637338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading DB descriptors from results/descriptors_db_hog.pkl\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# 1. Build or load database descriptors (HOG)\n",
    "\n",
    "if os.path.exists(DESC_DB_PATH):\n",
    "    print(f\"âœ… Loading DB descriptors from {DESC_DB_PATH}\")\n",
    "    with open(DESC_DB_PATH, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        db_descs = data[\"desc_gt\"]    # list of numpy arrays\n",
    "        db_names = data[\"gt_names\"]   # list of image filenames\n",
    "else:\n",
    "    print(\"ðŸ§  Computing HOG descriptors for database images...\")\n",
    "    db_names, db_imgs = load_images_from_folder(DB_FOLDER)\n",
    "    db_descs = []\n",
    "    for img, name in zip(db_imgs, db_names):\n",
    "        print(\"Processing image\",name)\n",
    "        d = compute_hog_descriptor(img)\n",
    "        db_descs.append(d)\n",
    "\n",
    "    # store only numpy arrays (serializable) â†’ no cv2.KeyPoint here\n",
    "    with open(DESC_DB_PATH, \"wb\") as f:\n",
    "        pickle.dump({\"desc_gt\": db_descs, \"gt_names\": db_names}, f)\n",
    "    print(f\"âœ… Saved DB descriptors to {DESC_DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65aa4876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading query images...\n",
      "ðŸ“¥ Loading ground-truth correspondences...\n",
      "â†’ 30 GT entries loaded\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 2. Load query images and ground-truth correspondences\n",
    "\n",
    "print(\"ðŸ“¥ Loading query images...\")\n",
    "q_names, q_imgs = load_images_from_folder(QUERY_FOLDER)\n",
    "\n",
    "print(\"ðŸ“¥ Loading ground-truth correspondences...\")\n",
    "with open(GT_PATH, \"rb\") as f:\n",
    "    gt_corresps = pickle.load(f)   # e.g. [[236], [107], [280, 285], ...]\n",
    "print(f\"â†’ {len(gt_corresps)} GT entries loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc048f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Computing HOG for queries...\n",
      "Processing image 00000.jpg\n",
      "Processing image 00001.jpg\n",
      "Processing image 00002.jpg\n",
      "Processing image 00003.jpg\n",
      "Processing image 00004.jpg\n",
      "Processing image 00005.jpg\n",
      "Processing image 00006.jpg\n",
      "Processing image 00007.jpg\n",
      "Processing image 00008.jpg\n",
      "Processing image 00009.jpg\n",
      "Processing image 00010.jpg\n",
      "Processing image 00011.jpg\n",
      "Processing image 00012.jpg\n",
      "Processing image 00013.jpg\n",
      "Processing image 00014.jpg\n",
      "Processing image 00015.jpg\n",
      "Processing image 00016.jpg\n",
      "Processing image 00017.jpg\n",
      "Processing image 00018.jpg\n",
      "Processing image 00019.jpg\n",
      "Processing image 00020.jpg\n",
      "Processing image 00021.jpg\n",
      "Processing image 00022.jpg\n",
      "Processing image 00023.jpg\n",
      "Processing image 00024.jpg\n",
      "Processing image 00025.jpg\n",
      "Processing image 00026.jpg\n",
      "Processing image 00027.jpg\n",
      "Processing image 00028.jpg\n",
      "Processing image 00029.jpg\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 3. Compute HOG descriptors for queries\n",
    "\n",
    "print(\"ðŸ§  Computing HOG for queries...\")\n",
    "desc_query = []\n",
    "for img, img_name in zip(q_imgs, q_names):\n",
    "    print(\"Processing image\",img_name)\n",
    "    # Split possible multiple artworks\n",
    "    splitted = split_images(img)\n",
    "\n",
    "    if splitted[0] is True:\n",
    "        splitted = splitted[1]  # two artworks detected\n",
    "        left_artwork, right_artwork = splitted\n",
    "\n",
    "        left_artwork = preprocess_image(left_artwork)\n",
    "        right_artwork = preprocess_image(right_artwork)\n",
    "\n",
    "        iml, left_mask, left_output, _ = background_remover.remove_background_morphological_gradient(left_artwork)\n",
    "        imr, right_mask, right_output, _ = background_remover.remove_background_morphological_gradient(right_artwork)\n",
    "\n",
    "        # Crop each artwork to its mask bounding box (no black borders)\n",
    "        left_cropped = background_remover.crop_to_mask_rectangle(left_artwork, left_mask)\n",
    "        right_cropped = background_remover.crop_to_mask_rectangle(right_artwork, right_mask)\n",
    "\n",
    "        # Extract descriptors\n",
    "        #kps_left = detect_keypoints(left_cropped, METHOD)\n",
    "        #kps_right = detect_keypoints(right_cropped, METHOD)\n",
    "        \n",
    "        desc_left  = compute_hog_descriptor(left_cropped)\n",
    "        desc_right = compute_hog_descriptor(right_cropped)\n",
    "\n",
    "        desc_query.append([desc_left, desc_right])\n",
    "        \n",
    "        \"\"\"plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(left_cropped, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Left Artwork: {img_name}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cv2.cvtColor(right_cropped, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Right Artwork: {img_name}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.show()\"\"\"\n",
    "\n",
    "    else:  # single artwork\n",
    "        splitted = splitted[1]  # single artwork\n",
    "        img = preprocess_image(splitted)\n",
    "        im, mask, output, _ = background_remover.remove_background_morphological_gradient(img)\n",
    "\n",
    "        # Crop to mask bounding box (remove black)\n",
    "        cropped = background_remover.crop_to_mask_rectangle(img, mask)\n",
    "\n",
    "        # Extract descriptor\n",
    "        #kps = detect_keypoints(cropped, METHOD)\n",
    "        desc = compute_hog_descriptor(cropped)\n",
    "            \n",
    "        desc_query.append([desc])  # keep structure consistent\n",
    "        \n",
    "        \"\"\"plt.imshow(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"{img_name}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "# stack DB descriptors into a matrix so cosine_similarity is fast\n",
    "db_mat = np.vstack(db_descs)   # shape (N_db, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a75854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… mAP@1 = 0.3857\n",
      "âœ… mAP@5 = 0.4000\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Compute mAP@1 and mAP@5 ---\n",
    "def compute_map_at_k(desc_query, desc_gt, gt_corresps, k=5):\n",
    "    \"\"\"\n",
    "    Compute mean Average Precision at K.\n",
    "    If a query image has multiple artworks, we pair each sub-descriptor with its matching GT index\n",
    "    (i.e., descs[j] -> gt_corresps[i][j]) when lengths match.\n",
    "    \"\"\"\n",
    "    aps = []\n",
    "\n",
    "    for i, descs in enumerate(desc_query):\n",
    "        q_gt = gt_corresps[i]\n",
    "        # Ensure list\n",
    "        if not isinstance(q_gt, list):\n",
    "            q_gt = [q_gt]\n",
    "\n",
    "        for desc in descs:\n",
    "            sims = cosine_similarity([desc], desc_gt)[0]\n",
    "            ranked_indices = np.argsort(-sims)[:k]\n",
    "\n",
    "            num_relevant = len(q_gt)\n",
    "            num_correct = 0\n",
    "            precision_at_i = []\n",
    "\n",
    "            for rank, idx in enumerate(ranked_indices, start=1):\n",
    "                if idx in q_gt:\n",
    "                    num_correct += 1\n",
    "                    precision_at_i.append(num_correct / rank)\n",
    "\n",
    "            ap = np.sum(precision_at_i) / num_relevant if num_relevant > 0 else 0\n",
    "            aps.append(ap)\n",
    "\n",
    "    return float(np.mean(aps)) if aps else 0.0\n",
    "\n",
    "\n",
    "\n",
    "map1 = compute_map_at_k(desc_query, db_descs, gt_corresps, k=1)\n",
    "map5 = compute_map_at_k(desc_query, db_descs, gt_corresps, k=5)\n",
    "\n",
    "print(f\"\\nâœ… mAP@1 = {map1:.4f}\")\n",
    "print(f\"âœ… mAP@5 = {map5:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
